@Article{Krippendorff.1970,
  author  = {Krippendorff, Klaus},
  journal = {Educational and Psychological Measurement},
  title   = {Estimating the Reliability, Systematic Error and Random Error of Interval Data},
  year    = {1970},
  issn    = {0013-1644},
  number  = {1},
  pages   = {61--70},
  volume  = {30},
  doi     = {10.1177/001316447003000105},
  groups  = {BIP},
}

@Article{Krippendorff.2008,
  author  = {Krippendorff, Klaus},
  journal = {Communication Methods and Measures},
  title   = {Systematic and Random Disagreement and the Reliability of Nominal Data},
  year    = {2008},
  issn    = {1931-2458},
  number  = {4},
  pages   = {323--338},
  volume  = {2},
  doi     = {10.1080/19312450802467134},
  groups  = {BIP},
}

@Article{Landis1977,
  author    = {Landis, J Richard and Koch, Gary G},
  journal   = {biometrics},
  title     = {The measurement of observer agreement for categorical data},
  year      = {1977},
  pages     = {159--174},
  groups    = {BIP},
  publisher = {JSTOR},
}

@Article{krippendorff2009testing,
  author    = {Krippendorff, Klaus},
  journal   = {The content analysis reader},
  title     = {Testing the reliability of content analysis data},
  year      = {2009},
  pages     = {350--357},
  groups    = {BIP},
  publisher = {Sage Thousand Oaks, CA},
}

@Article{Hayes2007,
  author    = {Hayes, Andrew F and Krippendorff, Klaus},
  journal   = {Communication methods and measures},
  title     = {Answering the call for a standard reliability measure for coding data},
  year      = {2007},
  number    = {1},
  pages     = {77--89},
  volume    = {1},
  groups    = {BIP},
  publisher = {Taylor \& Francis},
}

@Article{Krippendorff2004,
  author    = {Krippendorff, Klaus},
  journal   = {Human communication research},
  title     = {Reliability in content analysis: Some common misconceptions and recommendations},
  year      = {2004},
  number    = {3},
  pages     = {411--433},
  volume    = {30},
  groups    = {BIP},
  publisher = {Wiley Online Library},
}

@InProceedings{Igasheva2019,
  author    = {Anastasiia Sergeevna Igasheva},
  booktitle = {Education, innovation, research as a resource for community development},
  title     = {{LINGUISTIC} {PECULIARITIES} {OF} {PUN}, {ITS} {TYPOLOGY} {AND} {CLASSIFICATION}},
  year      = {2019},
  month     = {jul},
  publisher = {Publishing house Sreda},
  doi       = {10.31483/r-32974},
  groups    = {BIP},
}

@InProceedings{Dodge2017,
  author    = {Samuel Dodge and Lina Karam},
  booktitle = {2017 26th International Conference on Computer Communication and Networks ({ICCCN})},
  title     = {A Study and Comparison of Human and Deep Learning Recognition Performance under Visual Distortions},
  year      = {2017},
  month     = {jul},
  publisher = {{IEEE}},
  doi       = {10.1109/icccn.2017.8038465},
  groups    = {BIP},
}

@InProceedings{Galke2017,
  author    = {Galke, Lukas and Mai, Florian and Schelten, Alan and Brunsch, Dennis and Scherp, Ansgar},
  booktitle = {Proceedings of the Knowledge Capture Conference},
  title     = {Using Titles vs. Full-Text as Source for Automated Semantic Document Annotation},
  year      = {2017},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {K-CAP 2017},
  abstract  = {We conduct the first systematic comparison of automated semantic annotation based on either the full-text or only on the title metadata of documents. Apart from the prominent text classification baselines kNN and SVM, we also compare recent techniques of Learning to Rank and neural networks and revisit the traditional methods logistic regression, Rocchio, and Naive Bayes. Across three of our four datasets, the performance of the classifications using only titles reaches over 90% of the quality compared to the performance when using the full-text.},
  articleno = {20},
  doi       = {10.1145/3148011.3148039},
  groups    = {BIP},
  isbn      = {9781450355537},
  keywords  = {Multi-label classification, document analysis, semantic annotation},
  location  = {Austin, TX, USA},
  numpages  = {4},
  url       = {https://doi.org/10.1145/3148011.3148039},
}

@InProceedings{Ermakova2023,
  author    = {Ermakova, Liana and Miller, Tristan and Bosser, Anne-Gwenn and Palma Preciado, Victor Manuel and Sidorov, Grigori and Jatowt, Adam},
  booktitle = {Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2–6, 2023, Proceedings, Part III},
  title     = {Science For Fun: The CLEF 2023 JOKER Track On Automatic Wordplay Analysis},
  year      = {2023},
  address   = {Berlin, Heidelberg},
  pages     = {546–556},
  publisher = {Springer-Verlag},
  abstract  = {Understanding and translating humorous wordplay often requires recognition of implicit cultural references, knowledge of word formation processes, and discernment of double meanings – issues which pose challenges for humans and computers alike. This paper introduces the CLEF 2023 JOKER track, which takes an interdisciplinary approach to the creation of reusable test collections, evaluation metrics, and methods for the automatic processing of wordplay. We describe the track’s interconnected shared tasks for the detection, location, interpretation, and translation of puns. We also describe associated data sets and evaluation methodologies, and invite contributions making further use of our data.},
  doi       = {10.1007/978-3-031-28241-6_63},
  groups    = {BIP},
  isbn      = {978-3-031-28240-9},
  keywords  = {Wordplay generation, Machine translation, Puns, Humour, Wordplay, Wordplay interpretation, Wordplay detection},
  location  = {Dublin, Ireland},
  numpages  = {11},
  url       = {https://doi.org/10.1007/978-3-031-28241-6_63},
}

@InProceedings{GrosseBoelting2015,
  author    = {Gro\ss{}e-B\"{o}lting, Gregor and Nishioka, Chifumi and Scherp, Ansgar},
  booktitle = {Proceedings of the 8th International Conference on Knowledge Capture},
  title     = {A Comparison of Different Strategies for Automated Semantic Document Annotation},
  year      = {2015},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {K-CAP 2015},
  abstract  = {We introduce a framework for automated semantic document annotation that is composed of four processes, namely concept extraction, concept activation, annotation selection, and evaluation. The framework is used to implement and compare different annotation strategies motivated by the literature. For concept extraction, we apply entity detection with semantic hierarchical knowledge bases, Tri-gram, RAKE, and LDA. For concept activation, we compare a set of statistical, hierarchy-based, and graph-based methods. For selecting annotations, we compare top-k as well as kNN. In total, we define 43 different strategies including novel combinations like using graph-based activation with kNN. We have evaluated the strategies using three different datasets of varying size from three scientific disciplines (economics, politics, and computer science) that contain 100, 000 manually labeled documents in total. We obtain the best results on all three datasets by our novel combination of entity detection with graph-based activation (e.g., HITS and Degree) and kNN. For the economic and political science datasets, the best F-measure is .39 and .28, respectively. For the computer science dataset, the maximum F-measure of .33 can be reached. The experiments are the by far largest on scholarly content annotation, which typically are up to a few hundred documents per dataset only.},
  articleno = {8},
  doi       = {10.1145/2815833.2815838},
  groups    = {BIP},
  isbn      = {9781450338493},
  keywords  = {hierarchical knowledge bases, document annotation},
  location  = {Palisades, NY, USA},
  numpages  = {8},
  url       = {https://doi.org/10.1145/2815833.2815838},
}

 
@Misc{Luu2015,
  author   = {Luu, Chi},
  month    = oct,
  title    = {Linguistic {Anarchy}! {It}'s all {Pun} and {Games} {Until} {Somebody} {Loses} a {Sign}},
  year     = {2015},
  abstract = {The pun is in an interesting bind: it is both ubiquitous and reviled. We try to understand why.},
  groups   = {BIP},
  journal  = {JSTOR Daily},
  language = {en-US},
  url      = {https://daily.jstor.org/linguistic-anarchy-pun-games-somebody-loses-sign/},
  urldate  = {2023-05-10},
}

@Misc{blohm2020leveraging,
  author        = {Matthias Blohm and Marc Hanussek and Maximilien Kintz},
  title         = {Leveraging Automated Machine Learning for Text Classification: Evaluation of AutoML Tools and Comparison with Human Performance},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2012.03575},
  groups        = {BIP},
  primaryclass  = {cs.LG},
}
